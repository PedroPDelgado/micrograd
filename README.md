# micrograd

A replication of Andrej Karpathy's micrograd library.

This project is a minimalistic implementation of automatic differentiation and a simple neural network framework, built by closely following [Andrej Karpathyâ€™s micrograd tutorial](https://www.youtube.com/watch?v=VMj-3S1tku0). The goal is to understand the core principles behind backpropagation and how modern deep learning frameworks work under the hood.

The code was written alongside Karpathy's explanation, with the aim of reinforcing the concepts through hands-on implementation.

# Reference

ðŸŽ¥ [The spelled-out intro to neural networks and backpropagation](https://www.youtube.com/watch?v=VMj-3S1tku0): building micrograd by Andrej Karpathy.
